{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538bca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67e5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\Projects\\ThesisAI\\.venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora/cebuano_lora_epoch_model\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e82c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adc2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Translate this Cebuano sentence to English: Nalingaw kami sa pagdula sa basketball.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "We enjoyed playing basketball.\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Translate this Cebuano sentence to English: Nalingaw kami sa pagdula sa basketball.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\"\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b0615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a JSON object that contains random Cebuano story with comprehension questions and answers.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "{\n",
      "    \"title\": \"Ang Pagpaningkamot ni Juan\",\n",
      "    \"story\": \"Si Juan usa ka batang lalaki nga nagpuyo sa usa ka gamay nga baryo.\\nBisan pa man sa kalisod sa ilang kinabuhi, buotan ug andam siya sa pagpaningkamot.\\nUsa ka adlaw, naglakaw siya paingon sa uma aron magtanom og mga gulay.\\n\\\"Juan, nganong magtanom ka sa uma?\\\" pangutana ni Lola.\\n\\\"Lola, gusto ko nga makatabang sa pamilya, ug gusto ko usab nga makapalit og pagkaon,\\\" tubag ni Juan.\\n\\\"Maayo kaayo nga plano, Juan. Ang pagpaningkamot ug pagkugi magahatag og maayo nga resulta,\\\" ingon ni Lola.\\nNagtrabaho si Juan sa uma hangtod sa hapon, nagtanom og mga linga, kamatis, ug talong.\\n\\\"Lola, unsaon nato pag-atiman sa mga tanom?\\\" pangutana ni Juan.\\n\\\"Kinahanglan nato silang tubigon ug siguruhon nga walay mga hayop nga magkaon niini,\\\" tubag ni Lola.\\nPagkahuman sa ilang trabaho, naglingkod sila sa ilawom sa kahoy ug nagtan-aw sa mga tanom.\\n\\\"Lola, nakita ba nimo ang mga tanom nga nagtubo?\\\" pangutana ni Juan.\\n\\\"Oo, Juan. Ang imong paningkamot nagbunga. Tan-awa, ang mga tanom nagtubo na!\\\" tubag ni Lola nga nagtan-aw sa mga tanom.\\nNagpasalamat si Juan sa tabang ni Lola ug nagplano nga magpadayon sa pagpaningkamot.\\n\\\"Lola, magpadayon ko sa pagpaningkamot ug magtanom pa og daghang mga gulay,\\\" ingon ni Juan.\\n\\\"Maayo kaayo, Juan. Ang imong paningkamot magahatag og maayo nga resulta,\\\" ingon ni Lola.\\n\",\n",
      "    \"questions\": [\n",
      "        {\n",
      "        \"question\": \"Kinsa ang nagtanom og mga gulay sa uma?\",\n",
      "        \"choices\": [\"A. Si Juan\", \"B. Si Lola\", \"C. Si Juan ug Lola\", \"D. Ang mga silingan\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Si Juan ang nagtanom og mga gulay sa uma aron makatabang sa pamilya ug makapalit og pagkaon.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Unsa ang gipangayo ni Juan sa iyang Lola?\",\n",
      "        \"choices\": [\"A. Tabang sa pagtanom\", \"B. Tabang sa pagpangisda\", \"C. Tabang sa pag-ani\", \"D. Tabang sa pagpanglimpyo\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Gipangayo ni Juan ang tabang ni Lola sa pagtanom og mga gulay sa uma.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Kanus-a nagtrabaho si Juan sa uma?\",\n",
      "        \"choices\": [\"A. Sa buntag\", \"B. Sa hapon\", \"C. Sa gabii\", \"D. Sa udto\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Nagtrabaho si Juan sa uma hangtod sa hapon, nagtanom og mga gulay.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Asa nagtrabaho si Juan ug Lola?\",\n",
      "        \"choices\": [\"A. Sa uma\", \"B. Sa balay\", \"C. Sa merkado\", \"D. Sa simbahan\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Nagtrabaho si Juan ug Lola sa uma, nagtanom og mga gulay.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Ngano nga magtanom si Juan?\",\n",
      "        \"choices\": [\"A. Kay gusto siya magpahulay\", \"B. Kay gusto siya makatabang sa pamilya\", \"C. Kay gusto siya magdula\", \"D. Kay gusto siya magtudlo sa uban\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Si Juan nagtanom aron makatabang sa pamilya ug makapalit og pagkaon.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Unsa ang giingon ni Lola bahin sa paningkamot ni Juan?\",\n",
      "        \"choices\": [\"A. Dili siya magmalampuson\", \"B. Ang paningkamot magahatag og maayo nga resulta\", \"C. Magpuyo siya sa balay\", \"D. Magpuyo siya sa uma\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Giingon ni Lola nga ang paningkamot ni Juan magahatag og maayo nga resulta.\"\n",
      "    }\n",
      "    ]\n",
      "}\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "from transformers import TextStreamer\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a JSON object that contains random Cebuano story with comprehension questions and answers.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacb42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a JSON object that contains random Cebuano story with comprehension questions and answers.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "{\n",
      "    \"title\": \"Ang Panahon sa Pagpangandoy\",\n",
      "    \"story\": \"Si Pedro usa ka batang lalaki nga nagpuyo sa usa ka gamay nga baryo.\\nBisan pa sa kalisod, adunay dako nga pangandoy siya.\\n\\\"Pedro, ngano man nga nagapaningkamot ka?\\\" pangutana ni Inay.\\n\\\"Gusto ko magtudlo sa mga bata sa atong baryo, Inay,\\\" tubag ni Pedro.\\n\\\"Apan, Pedro, unsaon nimo pagkab-ot ang imong pangandoy?\\\" pangutana ni Inay nga nagtan-aw sa iyang anak.\\n\\\"Magtuon ko ug magpaningkamot, Inay. Ang akong pangandoy mao ang makatabang sa mga kabataan,\\\" ingon ni Pedro nga puno sa paglaum.\\nNagpadayon siya sa pag-eskwela ug nagsugod sa pag-apil sa mga seminar ug training nga makatabang sa iyang pangandoy.\\n\\\"Pedro, kinsa man ang imong inspirasyon?\\\" pangutana ni Lolo.\\n\\\"Si Lolo, Inay, ug ang akong mga ginikanan, sila ang nagtudlo nako nga ang edukasyon importante,\\\" tubag ni Pedro.\\nAng mga tawo sa baryo nagsuporta sa iyang pangandoy.\\n\\\"Pedro, maayo kaayo ka nga magtudlo, magpadayon lang ka,\\\" ingon ni Tiyo Juan.\\nNagpadayon si Pedro sa pagtuon ug pag-apil sa mga seminar.\\nPaglabay sa mga tuig, nakahuman siya ug nagtrabaho isip usa ka magtutudlo sa baryo.\\n\\\"Pedro, nakab-ot na nimo ang imong pangandoy?\\\" pangutana ni Inay.\\n\\\"Oo, Inay, nakatabang ko sa mga kabataan sa baryo. Nagtudlo ko sa mga bata ug nagtabang sa ilang edukasyon,\\\" tubag ni Pedro nga puno sa kalipay.\\nNagpasalamat siya sa tanan nga nagtabang kaniya ug nagpadayon sa pagtabang sa uban.\\n\",\n",
      "    \"questions\": [\n",
      "        {\n",
      "        \"question\": \"Kinsa ang nagtan-aw sa kalipay ni Pedro?\",\n",
      "        \"choices\": [\"A. Si Tiyo Juan\", \"B. Si Inay\", \"C. Si Lolo\", \"D. Ang mga bata\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Si Inay ang nagtan-aw sa kalipay ni Pedro ug nagpasalamat sa iyang mga paningkamot.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Unsa ang pangandoy ni Pedro?\",\n",
      "        \"choices\": [\"A. Magtukod og negosyo\", \"B. Magtudlo sa mga bata\", \"C. Magpuyo sa syudad\", \"D. Magbansay og sports\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Ang pangandoy ni Pedro mao ang magtudlo sa mga bata sa baryo aron makatabang sa ilang edukasyon.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Kanus-a nagsugod si Pedro sa pag-apil sa mga seminar?\",\n",
      "        \"choices\": [\"A. Sa dihang nagsugod siya og trabaho\", \"B. Sa dihang nag-eskwela siya\", \"C. Sa dihang nag-apil siya sa mga seminar\", \"D. Sa dihang nagpadayon siya sa pagtuon\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Nagsugod si Pedro sa pag-apil sa mga seminar ug training samtang nag-eskwela siya aron makab-ot ang iyang pangandoy.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Asa nagpuyo si Pedro?\",\n",
      "        \"choices\": [\"A. Sa syudad\", \"B. Sa baryo\", \"C. Sa bukid\", \"D. Sa baybayon\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Si Pedro nagpuyo sa usa ka gamay nga baryo ug didto siya nagpaningkamot aron makab-ot ang iyang pangandoy.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Ngano nga nagapaningkamot si Pedro?\",\n",
      "        \"choices\": [\"A. Aron magtukod og negosyo\", \"B. Aron magtudlo sa mga bata\", \"C. Aron magpuyo sa syudad\", \"D. Aron magtrabaho sa gobyerno\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Nagapaningkamot si Pedro aron magtudlo sa mga bata sa baryo ug makatabang sa ilang edukasyon.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Unsa ang gipasabot ni Pedro sa iyang inspirasyon?\",\n",
      "        \"choices\": [\"A. Ang iyang mga ginikanan\", \"B. Ang iyang mga kauban sa trabaho\", \"C. Ang mga bata sa baryo\", \"D. Ang mga magtutudlo sa syudad\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Si Pedro nagpasabot nga ang iyang inspirasyon mao ang iyang mga ginikanan nga nagtudlo kaniya nga ang edukasyon importante.\"\n",
      "    }\n",
      "    ]\n",
      "}\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "from transformers import TextStreamer\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a JSON object that contains random Cebuano story with comprehension questions and answers.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31160e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a JSON object that contains random Cebuano story with comprehension questions and answers.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "{\n",
      "    \"title\": \"Ang Pagpaningkamot ni Tomas\",\n",
      "    \"story\": \"Si Tomas usa ka batang lalaki nga nagpuyo sa baryo. \\nUsa ka adlaw, naglakaw siya paingon sa uma aron magtabang sa iyang amahan. \\n\\\"Tomas, tabangi ko sa pag-ani sa mais,\\\" ingon sa iyang amahan. \\n\\\"Oo, amahan, magpaningkamot ko,\\\" tubag ni Tomas. \\nSamtang nagtrabaho sila sa uma, nakakita sila og usa ka gamay nga hayop nga nagdagan. \\n\\\"Unsa kaha kini nga hayop?\\\" pangutana ni Tomas. \\n\\\"Kana usa ka usa ka usa, Tomas. Magdala kini og kalipay sa uma,\\\" tubag sa iyang amahan. \\nNagpadayon sila sa pag-ani hangtod sa hapon. \\n\\\"Tomas, magpahulay na ta kay magkaon na ta,\\\" ingon sa amahan. \\n\\\"Oo, amahan. Lami kaayo ang pagkaon nga atong gihimo,\\\" tubag ni Tomas. \\nPagkahuman sa ilang panihapon, nagtan-aw sila sa kahayag sa adlaw nga nagtabon sa uma. \\n\\\"Tomas, makita nimo nga ang paningkamot magahatag og maayo nga resulta?\\\" pangutana sa amahan. \\n\\\"Oo, amahan. Ang pagpaningkamot magahatag og kalipay ug kahulugan sa atong kinabuhi,\\\" tubag ni Tomas. \\nNagpasalamat si Tomas sa iyang amahan sa pagtudlo kaniya sa kahulugan sa paningkamot. \\nNaglakaw sila pabalik sa balay, nagdala sa mga bunga sa ilang paningkamot. \\n\\\"Tomas, magpadayon ta sa pagpaningkamot aron magmalampuson,\\\" ingon sa amahan. \\n\\\"Oo, amahan. Magpadayon ko sa pagpaningkamot sa tanan nga akong buhaton,\\\" tubag ni Tomas.\",\n",
      "    \"questions\": [\n",
      "        {\n",
      "        \"question\": \"Kinsa ang nagtudlo ni Tomas sa kahulugan sa paningkamot?\",\n",
      "        \"choices\": [\"A. Ang iyang inahan\", \"B. Ang iyang igsoon\", \"C. Ang iyang amahan\", \"D. Ang iyang lolo\"],\n",
      "        \"answer\": \"C\",\n",
      "        \"explanation\": \"Ang iyang amahan ang nagtudlo ni Tomas sa kahulugan sa paningkamot ug sa kahayag sa trabaho.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Unsa ang gihimo ni Tomas ug sa iyang amahan sa uma?\",\n",
      "        \"choices\": [\"A. Nag-ani sa mais\", \"B. Nagbantay sa mga hayop\", \"C. Nagdula sa uma\", \"D. Nagtanom og mga kahoy\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Si Tomas ug ang iyang amahan nagtrabaho sa uma aron mag-ani sa mais.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Kanus-a sila nagpahulay?\",\n",
      "        \"choices\": [\"A. Sa buntag\", \"B. Sa hapon\", \"C. Sa gabii\", \"D. Sa udto\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Nagpahulay sila sa hapon human sa ilang trabaho sa uma.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Asa sila nagtan-aw sa kahayag sa adlaw?\",\n",
      "        \"choices\": [\"A. Sa uma\", \"B. Sa balay\", \"C. Sa suba\", \"D. Sa bukid\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Nagtan-aw sila sa kahayag sa adlaw nga nagtabon sa uma samtang nagpahulay.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Ngano nga magpadayon si Tomas sa pagpaningkamot?\",\n",
      "        \"choices\": [\"A. Kay gusto siya magdula\", \"B. Kay gusto siya magmalampuson\", \"C. Kay gusto siya magpahulay\", \"D. Kay gusto siya magtudlo\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Si Tomas magpadayon sa pagpaningkamot aron magmalampuson sa iyang mga buhat.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Giunsa ni Tomas ang pagtan-aw sa kahayag sa adlaw?\",\n",
      "        \"choices\": [\"A. Nagtan-aw siya sa kahayag sa adlaw nga nagtabon sa uma\", \"B. Nagdula siya sa kahayag sa adlaw\", \"C. Nagpahulay siya sa kahayag sa adlaw\", \"D. Nagbantay siya sa kahayag sa adlaw\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Nagtan-aw si Tomas sa kahayag sa adlaw nga nagtabon sa uma samtang nagpahulay.\"\n",
      "    }\n",
      "    ]\n",
      "}\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "from transformers import TextStreamer\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a JSON object that contains random Cebuano story with comprehension questions and answers.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d5b94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a JSON object that contains a Cebuano story about a boy who likes to eat with comprehension questions and answers.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "{\n",
      "    \"title\": \"Ang Bata nga Gihigugma ang Pagkaon\",\n",
      "    \"story\": \"Si Juan usa ka batang lalaki nga nagpuyo sa baryo.\\nBisan pa sa kalisod sa ilang pamilya, dili siya magpasipala sa pagkaon.\\nUsa ka adlaw, naglakaw siya paingon sa merkado aron mangita og pagkaon.\\n\\\"Juan, asa ka paingon?\\\" pangutana ni Lola.\\n\\\"Mangita ko og pagkaon, Lola,\\\" tubag ni Juan.\\nPag-abot niya sa merkado, nakakita siya og mga tindahan nga nagbaligya og prutas ug gulay.\\n\\\"Lola, gusto ko magka-apples ug mangga!\\\" sigaw ni Juan.\\n\\\"Ayaw kalimti ang mga prutas, Juan. Maayo sila para sa imong lawas,\\\" ingon ni Lola.\\nNagpalit siya og mga prutas ug nagdala sa mga basket.\\nPagbalik niya sa balay, nag-andam siya og panihapon.\\n\\\"Lola, magluto ta og sinigang nga baboy!\\\" ingon ni Juan.\\n\\\"Oo, Juan, magluto ta. Pero kinahanglan nato mag-amping sa pagluto,\\\" tubag ni Lola.\\nSamtang nagluto, nag-istorya sila bahin sa ilang mga hilig.\\n\\\"Gusto ko magbasa og libro, Lola,\\\" ingon ni Juan.\\n\\\"Maayo na, Juan. Ang pagbasa makatabang sa imong kahibalo,\\\" tubag ni Lola.\\nPagkahuman sa panihapon, naglingkod sila sa sala ug nagtan-aw sa telebisyon.\\n\\\"Lola, magtan-aw ta og mga programa!\\\" ingon ni Juan.\\n\\\"Oo, Juan, magtan-aw ta. Pero magbantay lang ta sa oras,\\\" tubag ni Lola.\\nSa katapusan, nagpasalamat si Juan sa iyang Lola sa pagkaon ug sa mga istorya nga ilang gisugdan.\\n\\\"Lola, salamat sa imong mga tambag ug pag-atiman,\\\" ingon ni Juan.\\n\\\"Walay sapayan, Juan. Magpadayon ta sa pagpaningkamot,\\\" tubag ni Lola.\\n\",\n",
      "    \"questions\": [\n",
      "        {\n",
      "        \"question\": \"Kinsa ang batang lalaki nga gihigugma ang pagkaon?\",\n",
      "        \"choices\": [\"A. Si Pedro\", \"B. Si Juan\", \"C. Si Luis\", \"D. Si Carlos\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Si Juan ang batang lalaki nga gihigugma ang pagkaon, ingon sa sugilanon.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Unsa ang gipalit ni Juan sa merkado?\",\n",
      "        \"choices\": [\"A. Prutas ug gulay\", \"B. Isda\", \"C. Gatas\", \"D. Saging\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Nakakita si Juan og mga tindahan nga nagbaligya og prutas ug gulay sa merkado.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Kanus-a nagluto si Juan ug Lola?\",\n",
      "        \"choices\": [\"A. Sa buntag\", \"B. Sa hapon\", \"C. Sa gabii\", \"D. Sa udto\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Nagluto sila sa hapon human ni Juan magpalit og pagkaon sa merkado.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Asa nagpuyo si Juan?\",\n",
      "        \"choices\": [\"A. Sa syudad\", \"B. Sa baryo\", \"C. Sa bukid\", \"D. Sa baybayon\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Si Juan nagpuyo sa baryo, ingon sa sugilanon.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Ngano nga gusto ni Juan magbasa og libro?\",\n",
      "        \"choices\": [\"A. Para magtudlo siya\", \"B. Para makatabang sa iyang kahibalo\", \"C. Para magdula\", \"D. Para maghimo og proyekto\"],\n",
      "        \"answer\": \"B\",\n",
      "        \"explanation\": \"Gusto ni Juan magbasa og libro aron makatabang sa iyang kahibalo, ingon sa sugilanon.\"\n",
      "    },\n",
      "        {\n",
      "        \"question\": \"Unsa ang gipangayo ni Juan sa iyang Lola pagkahuman sa panihapon?\",\n",
      "        \"choices\": [\"A. Magtan-aw og programa\", \"B. Magluto og laing pagkaon\", \"C. Magbasa og libro\", \"D. Maghimo og proyekto\"],\n",
      "        \"answer\": \"A\",\n",
      "        \"explanation\": \"Pagkahuman sa panihapon, nagpasalamat si Juan ug nagpasabot nga magtan-aw sila og mga programa sa telebisyon.\"\n",
      "    }\n",
      "    ]\n",
      "}\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "from transformers import TextStreamer\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Generate a JSON object that contains a Cebuano story about a boy who likes to eat with comprehension questions and answers.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06338976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 13.61 out of 31.79 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:34<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at save_q4/model into bf16 GGUF format.\n",
      "The output location will be c:\\Users\\user\\Documents\\Projects\\ThesisAI\\save_q4\\model\\unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: model\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:c:\\Users\\user\\Documents\\Projects\\ThesisAI\\save_q4\\model\\unsloth.BF16.gguf: n_tensors = 292, total_size = 16.1G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.1G/16.1G [01:44<00:00, 154Mbyte/s] \n",
      "INFO:hf-to-gguf:Model successfully exported to c:\\Users\\user\\Documents\\Projects\\ThesisAI\\save_q4\\model\\unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: c:\\Users\\user\\Documents\\Projects\\ThesisAI\\save_q4\\model\\unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\nYou do not need to close this Python program. Run the following commands in a new terminal:\nYou must run this in the same folder as you're saving your model.\ngit clone --recursive https://github.com/ggerganov/llama.cpp\ncd llama.cpp && make clean && make all -j\nOnce that's done, redo the quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_q4/model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mq4_k_m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Documents\\Projects\\ThesisAI\\.venv\\Lib\\site-packages\\unsloth\\save.py:1857\u001b[39m, in \u001b[36munsloth_save_pretrained_gguf\u001b[39m\u001b[34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   1854\u001b[39m is_sentencepiece_model = check_if_sentencepiece_model(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# Save to GGUF\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m all_file_locations, want_full_precision = \u001b[43msave_to_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_sentencepiece_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_save_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmakefile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[38;5;66;03m# Save Ollama modelfile\u001b[39;00m\n\u001b[32m   1863\u001b[39m modelfile = create_ollama_modelfile(tokenizer, all_file_locations[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Documents\\Projects\\ThesisAI\\.venv\\Lib\\site-packages\\unsloth\\save.py:1254\u001b[39m, in \u001b[36msave_to_gguf\u001b[39m\u001b[34m(model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, _run_installer)\u001b[39m\n\u001b[32m   1243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1244\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Quantization failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1245\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mYou are in a Kaggle environment, which might be the reason this is failing.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   (...)\u001b[39m\u001b[32m   1251\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mI suggest you to save the 16bit model first, then use manual llama.cpp conversion.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1252\u001b[39m             )\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1255\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1256\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou do not need to close this Python program. Run the following commands in a new terminal:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1257\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou must run this in the same folder as you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre saving your model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1258\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgit clone --recursive https://github.com/ggerganov/llama.cpp\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1259\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcd llama.cpp && make clean && make all -j\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1260\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOnce that\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms done, redo the quantization.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1261\u001b[39m         )\n\u001b[32m   1262\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.\nYou do not need to close this Python program. Run the following commands in a new terminal:\nYou must run this in the same folder as you're saving your model.\ngit clone --recursive https://github.com/ggerganov/llama.cpp\ncd llama.cpp && make clean && make all -j\nOnce that's done, redo the quantization."
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"save_q4/model\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b6e635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n",
      "make: *** No rule to make target 'clean'.  Stop.\n",
      "make: *** No rule to make target 'all'.  Stop.\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp\n",
    "!make clean\n",
    "!make all -j\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
